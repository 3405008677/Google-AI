"""
Supervisor Architecture - Worker Implementations

å…·ä½“çš„ Worker å®ç°ï¼Œè¿™äº›æ˜¯ Layer 4 çš„ä¸“å®¶å›¢é˜Ÿã€‚

Worker ç±»å‹ï¼š
1. Researcher: æœç´¢ä¸è°ƒç ”ä¸“å®¶
2. DataAnalyst: æ•°æ®åˆ†æä¸“å®¶
3. Writer: å†…å®¹åˆ›ä½œä¸“å®¶
4. General: é€šç”¨åŠ©æ‰‹

åŠ¨æ€æ¨¡å‹é€‰æ‹©ï¼š
Workers ä¼šæ ¹æ® user_context è‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„ AI æ¨¡å‹ã€‚
"""

from typing import Dict, Any, List, Optional
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.language_models import BaseChatModel

from src.router.agents.supervisor.registry import (
    Worker, 
    WorkerType, 
    BaseWorkerMixin,
)
from src.router.agents.supervisor.state import SupervisorState, create_thinking_step
from src.router.agents.supervisor.llm_factory import create_llm_from_state
from src.server.logging_setup import logger

# ä½¿ç”¨æ–°çš„å…¬å…±æ¨¡ç»„
from src.common.prompts import get_prompt
from src.common.function_calls import get_tools_for_langchain, get_tool_executor

# å·¥å…·å¯¼å…¥
from src.tools import get_tavily_search, is_tavily_configured, get_datetime_tool

# Function Call é™çº§æ–¹æ¡ˆ
from src.router.agents.supervisor.function_call import get_fallback_manager


class BaseWorker(Worker, BaseWorkerMixin):
    """
    Worker åŸºç±»
    
    æä¾›æ‰€æœ‰ Worker å…±ç”¨çš„åŠŸèƒ½ï¼Œå‡å°‘é‡å¤ä»£ç ã€‚
    """
    
    def __init__(
        self,
        name: str,
        description: str,
        priority: int = 0,
        worker_type: WorkerType = WorkerType.LLM_POWERED,
        default_temperature: float = 0.5,
    ):
        super().__init__(
            name=name,
            description=description,
            priority=priority,
            worker_type=worker_type,
        )
        self.default_temperature = default_temperature
    
    def get_llm(self, state: SupervisorState, temperature: Optional[float] = None) -> BaseChatModel:
        """æ ¹æ®ç”¨æˆ·ä¸Šä¸‹æ–‡è·å–å¯¹åº”çš„ LLM"""
        temp = temperature if temperature is not None else self.default_temperature
        return create_llm_from_state(state, temperature=temp)
    
    def get_query(self, state: SupervisorState) -> Optional[str]:
        """è·å–ç”¨æˆ·æŸ¥è¯¢"""
        messages = state.get("messages", [])
        return self.get_original_query(state) or self.get_last_user_query(messages)
    
    def get_task_hint(self, state: SupervisorState) -> str:
        """è·å–å½“å‰ä»»åŠ¡æè¿°çš„æç¤º"""
        current_step = self.get_current_task_step(state)
        if current_step:
            description = current_step.get("description", "")
            if description:
                return f"ä»»åŠ¡è¦æ±‚ï¼š{description}\n\n"
        return ""
    
    def log_start(self, emoji: str = "ğŸ”§") -> None:
        """è®°å½•ä»»åŠ¡å¼€å§‹æ—¥å¿—"""
        logger.info(f"{emoji} [{self.name}] å¼€å§‹æ‰§è¡Œä»»åŠ¡")
        self._execution_count += 1


class ResearcherWorker(BaseWorker):
    """
    ç ”ç©¶ä¸“å®¶ Worker
    
    è´Ÿè´£æœç´¢å’Œæ”¶é›†ä¿¡æ¯ã€‚
    æ”¯æŒï¼šWeb æœç´¢ï¼ˆä½¿ç”¨ Tavily APIï¼‰ã€é˜…è¯»å’Œæ‘˜è¦ã€è¿½é—®æœç´¢
    """
    
    def __init__(self, search_tool=None):
        super().__init__(
            name="Researcher",
            description="æœç´¢ä¸“å®¶ï¼Œæ“…é•¿åœ¨äº’è”ç½‘ä¸ŠæŸ¥æ‰¾å’Œæ”¶é›†ä¿¡æ¯ã€‚å¯ä»¥è¿›è¡Œå¤šè½®æœç´¢å’Œä¿¡æ¯æ•´åˆï¼Œå›ç­”å…³äºäº‹å®ã€æ•°æ®ã€æ–°é—»ç­‰é—®é¢˜ã€‚",
            priority=10,
            default_temperature=0.3,
        )
        self.search_tool = search_tool
        self._tavily_configured = is_tavily_configured()
    
    async def _web_search(self, query: str) -> str:
        """æ‰§è¡Œ Web æœç´¢"""
        # 1. å¦‚æœæœ‰ä¼ å…¥çš„æœç´¢å·¥å…·ï¼Œä½¿ç”¨å®ƒ
        if self.search_tool:
            try:
                results = await self.search_tool.ainvoke(query)
                return str(results)
            except Exception as e:
                logger.warning(f"æœç´¢å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
        
        # 2. ä½¿ç”¨ Tavily æœç´¢ï¼ˆå¦‚æœå·²é…ç½®ï¼‰
        if self._tavily_configured:
            try:
                tavily = get_tavily_search()
                return await tavily.ainvoke(query)
            except Exception as e:
                logger.warning(f"Tavily æœç´¢å¤±è´¥: {e}")
        
        # 3. é™çº§æ–¹æ¡ˆ
        logger.warning(f"ğŸ” [{self.name}] Tavily æœªé…ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæœç´¢")
        return f"å…³äº '{query}' çš„æœç´¢ç»“æœï¼š[Tavily æœªé…ç½®ï¼Œè¯·è®¾ç½® TAVILY_API_KEY ç¯å¢ƒå˜é‡ä»¥å¯ç”¨è”ç½‘æœç´¢]"
    
    async def execute(self, state: SupervisorState) -> Dict[str, Any]:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        self.log_start("ğŸ”")
        
        query = self.get_query(state)
        if not query:
            return self._create_response("æ²¡æœ‰æ”¶åˆ°éœ€è¦ç ”ç©¶çš„é—®é¢˜ã€‚", state)
        
        try:
            # æ‰§è¡Œæœç´¢
            search_results = await self._web_search(query)
            
            # ä½¿ç”¨ LLM åˆ†ææœç´¢ç»“æœ
            system_prompt = get_prompt("workers.researcher.system")
            human_prompt = get_prompt("workers.researcher.human")
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", human_prompt),
            ])
            
            llm = self.get_llm(state)
            chain = prompt | llm
            result = await chain.ainvoke({
                "query": query,
                "task_hint": self.get_task_hint(state),
                "search_results": search_results,
            })
            
            content = result.content if hasattr(result, 'content') else str(result)
            
            return self.create_worker_response(
                worker_name=self.name,
                content=content,
                state=state,
                thinking_step=create_thinking_step(
                    step_type="reasoning",
                    content="å®Œæˆæœç´¢å’Œåˆ†æä»»åŠ¡",
                    worker=self.name,
                ),
            )
            
        except Exception as e:
            logger.error(f"[{self.name}] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return self.create_error_response(
                worker_name=self.name,
                error_message=f"ç ”ç©¶ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}",
                state=state,
            )


class DataAnalystWorker(BaseWorker):
    """
    æ•°æ®åˆ†æä¸“å®¶ Worker
    
    è´Ÿè´£æ•°æ®æŸ¥è¯¢å’Œåˆ†æã€‚
    """
    
    def __init__(self):
        super().__init__(
            name="DataAnalyst",
            description="æ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿æŸ¥è¯¢ä¸šåŠ¡æ•°æ®åº“ã€åˆ†æé”€å”®/åº“å­˜/ç”¨æˆ·ç­‰ä¸šåŠ¡æ•°æ®è¶‹åŠ¿ã€ç”Ÿæˆæ•°æ®æŠ¥å‘Šã€‚ã€æ³¨æ„ã€‘ä¸è´Ÿè´£å›ç­”å½“å‰æ—¥æœŸã€æ—¶é—´ç­‰ç³»ç»Ÿä¿¡æ¯é—®é¢˜ï¼Œè¿™ç±»é—®é¢˜è¯·äº¤ç»™ Generalã€‚",
            priority=10,
            default_temperature=0.1,
        )
    
    async def execute(self, state: SupervisorState) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åˆ†æä»»åŠ¡"""
        self.log_start("ğŸ“Š")
        
        query = self.get_query(state)
        if not query:
            return self._create_response("æ²¡æœ‰æ”¶åˆ°éœ€è¦åˆ†æçš„æ•°æ®é—®é¢˜ã€‚", state)
        
        try:
            system_prompt = get_prompt("workers.data_analyst.system")
            human_prompt = get_prompt("workers.data_analyst.human")
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", human_prompt),
            ])
            
            llm = self.get_llm(state)
            chain = prompt | llm
            result = await chain.ainvoke({
                "query": query,
                "task_hint": self.get_task_hint(state),
            })
            
            content = result.content if hasattr(result, 'content') else str(result)
            
            return self.create_worker_response(
                worker_name=self.name,
                content=content,
                state=state,
                thinking_step=create_thinking_step(
                    step_type="reasoning",
                    content="å®Œæˆæ•°æ®åˆ†æä»»åŠ¡",
                    worker=self.name,
                ),
            )
            
        except Exception as e:
            logger.error(f"[{self.name}] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return self.create_error_response(
                worker_name=self.name,
                error_message=f"æ•°æ®åˆ†æä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}",
                state=state,
            )


class WriterWorker(BaseWorker):
    """
    æ–‡æ¡ˆä¸“å®¶ Worker
    
    è´Ÿè´£æ’°å†™å’Œæ€»ç»“ï¼Œå¯ä»¥æ•´åˆå…¶ä»– Worker çš„ç»“æœç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚
    """
    
    def __init__(self):
        super().__init__(
            name="Writer",
            description="æ–‡æ¡ˆä¸“å®¶ï¼Œæ“…é•¿æ’°å†™æŠ¥å‘Šã€æ€»ç»“ä¿¡æ¯ã€æ•´ç†æ–‡æ¡£ã€‚å¯ä»¥æ•´åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯ï¼Œæ ¹æ®ç”¨æˆ·è¯­æ°”åå¥½ç”Ÿæˆç»“æ„åŒ–çš„æœ€ç»ˆè¾“å‡ºï¼ˆMarkdown/è¡¨æ ¼ï¼‰ã€‚",
            priority=5,
            default_temperature=0.7,
        )
    
    async def execute(self, state: SupervisorState) -> Dict[str, Any]:
        """æ‰§è¡Œæ–‡æ¡ˆæ’°å†™ä»»åŠ¡"""
        self.log_start("âœï¸")
        
        messages = state.get("messages", [])
        worker_outputs = self.get_worker_outputs(messages)
        original_query = self.get_original_query(state)
        user_context = self.get_user_context(state)
        language = user_context.get("language", "zh-CN")
        
        if not worker_outputs and not original_query:
            return self._create_response("æ²¡æœ‰å¯ç”¨çš„ä¿¡æ¯æ¥æ’°å†™å†…å®¹ã€‚", state)
        
        try:
            # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = ""
            if worker_outputs:
                context_info = "\n\n".join([
                    f"### {output['name']} çš„è¾“å‡ºï¼š\n{output['content']}"
                    for output in worker_outputs
                ])
            
            system_prompt = get_prompt("workers.writer.system", language="{language}")
            human_prompt = get_prompt("workers.writer.human")
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", human_prompt),
            ])
            
            llm = self.get_llm(state)
            chain = prompt | llm
            result = await chain.ainvoke({
                "query": original_query or "æ•´åˆç°æœ‰ä¿¡æ¯",
                "task_hint": self.get_task_hint(state),
                "context": context_info or "æ— é¢å¤–ä¿¡æ¯",
                "language": "ä¸­æ–‡" if "zh" in language else "English",
            })
            
            content = result.content if hasattr(result, 'content') else str(result)
            
            return self.create_worker_response(
                worker_name=self.name,
                content=content,
                state=state,
                thinking_step=create_thinking_step(
                    step_type="reasoning",
                    content=f"å®Œæˆæ–‡æ¡ˆæ’°å†™ä»»åŠ¡ï¼Œæ•´åˆäº† {len(worker_outputs)} ä¸ªä¿¡æ¯æº",
                    worker=self.name,
                ),
            )
            
        except Exception as e:
            logger.error(f"[{self.name}] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return self.create_error_response(
                worker_name=self.name,
                error_message=f"æ–‡æ¡ˆæ’°å†™ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}",
                state=state,
            )


class GeneralWorker(BaseWorker):
    """
    é€šç”¨ Worker
    
    å¤„ç†ä¸€èˆ¬æ€§çš„å¯¹è¯å’Œä»»åŠ¡ã€‚
    æ”¯æŒ Function Calling æ¥è·å–å®æ—¶ä¿¡æ¯ï¼ˆå¦‚å½“å‰æ—¶é—´ï¼‰ã€‚
    å¦‚æœæ¨¡å‹ä¸æ”¯æŒ toolsï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°ç›´æ¥æ³¨å…¥æ—¶é—´çš„æ–¹å¼ã€‚
    """
    
    # å·¥å…·æ‰§è¡Œå™¨æ˜ å°„
    TOOL_EXECUTORS = {
        "get_current_datetime": lambda params: get_datetime_tool().invoke(params),
    }
    
    def __init__(self):
        super().__init__(
            name="General",
            description="é€šç”¨åŠ©æ‰‹ï¼Œå¯ä»¥å¤„ç†å„ç§ä¸€èˆ¬æ€§çš„å¯¹è¯å’Œä»»åŠ¡ã€‚ã€é‡è¦ã€‘è´Ÿè´£å›ç­”å…³äºå½“å‰æ—¥æœŸã€æ—¶é—´ã€æ˜ŸæœŸå‡ ç­‰æ—¶é—´ç›¸å…³é—®é¢˜ã€‚ä¹Ÿé€‚åˆå¤„ç†ç®€å•é—®ç­”ã€é—²èŠã€èº«ä»½ä»‹ç»ç­‰åœºæ™¯ã€‚",
            priority=1,
            default_temperature=0.5,
        )
        self._tools_supported = True
    
    def _get_tools(self) -> List[Dict[str, Any]]:
        """è·å– LangChain æ ¼å¼çš„å·¥å…·å®šä¹‰"""
        try:
            return get_tools_for_langchain(["get_current_datetime"])
        except Exception as e:
            logger.warning(f"[{self.name}] è·å–å·¥å…·å®šä¹‰å¤±è´¥: {e}")
            return []
    
    async def _execute_tool(self, tool_name: str, tool_args: Dict[str, Any]) -> str:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        executor = self.TOOL_EXECUTORS.get(tool_name)
        if executor:
            logger.info(f"ğŸ”§ [{self.name}] è°ƒç”¨å·¥å…·: {tool_name}")
            return executor(tool_args)
        return f"æœªçŸ¥å·¥å…·: {tool_name}"
    
    async def _execute_with_tools(
        self, 
        llm: BaseChatModel, 
        prompt: ChatPromptTemplate,
        query: str,
        history_messages: List,
        language: str,
        system_prompt: str,
    ) -> str:
        """ä½¿ç”¨ Function Calling æ‰§è¡Œ"""
        tools = self._get_tools()
        if not tools:
            raise ValueError("No tools available")
        
        llm_with_tools = llm.bind_tools(tools)
        chain = prompt | llm_with_tools
        
        result = await chain.ainvoke({
            "query": query,
            "history": history_messages,
            "language": language,
        })
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if hasattr(result, 'tool_calls') and result.tool_calls:
            logger.info(f"[{self.name}] LLM è¯·æ±‚è°ƒç”¨ {len(result.tool_calls)} ä¸ªå·¥å…·")
            
            tool_results = []
            for tool_call in result.tool_calls:
                tool_name = tool_call.get("name", "")
                tool_args = tool_call.get("args", {})
                tool_result = await self._execute_tool(tool_name, tool_args)
                tool_results.append({"tool": tool_name, "result": tool_result})
            
            # æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æ¶ˆæ¯
            from langchain_core.messages import ToolMessage
            
            tool_messages = []
            for i, tool_call in enumerate(result.tool_calls):
                tool_messages.append(ToolMessage(
                    content=tool_results[i]["result"],
                    tool_call_id=tool_call.get("id", f"tool_{i}"),
                ))
            
            # ç¬¬äºŒæ¬¡è°ƒç”¨
            final_prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{query}"),
                result,
                *tool_messages,
            ])
            
            final_chain = final_prompt | llm
            final_result = await final_chain.ainvoke({
                "query": query,
                "history": history_messages,
                "language": language,
            })
            
            return final_result.content if hasattr(final_result, 'content') else str(final_result)
        
        return result.content if hasattr(result, 'content') else str(result)
    
    async def _execute_without_tools(
        self,
        llm: BaseChatModel,
        query: str,
        history_messages: List,
        language: str,
        timezone: str,
    ) -> str:
        """
        ä¸ä½¿ç”¨ Function Calling æ‰§è¡Œï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        
        æ ¹æ®éœ€è¦çš„å·¥å…·ç±»å‹ï¼Œæ”¶é›†ç›¸åº”çš„é™çº§ä¿¡æ¯å¹¶æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­ã€‚
        å½“å‰æ”¯æŒï¼šdatetimeï¼ˆæ—¶é—´ä¿¡æ¯ï¼‰
        æœªæ¥å¯æ‰©å±•ï¼šsearchï¼ˆæœç´¢èƒ½åŠ›ï¼‰ã€data_queryï¼ˆæ•°æ®æŸ¥è¯¢ï¼‰ç­‰
        """
        logger.info(f"[{self.name}] ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼ˆç›´æ¥æ³¨å…¥å®æ—¶ä¿¡æ¯ï¼‰")
        
        # è·å–é™çº§æ–¹æ¡ˆç®¡ç†å™¨
        fallback_manager = get_fallback_manager()
        
        # ç¡®å®šéœ€è¦çš„é™çº§æ–¹æ¡ˆï¼ˆæ ¹æ®å·¥å…·åˆ—è¡¨ï¼‰
        # å½“å‰ General Worker åªéœ€è¦æ—¶é—´ä¿¡æ¯
        required_fallbacks = ["datetime"]
        
        # æ”¶é›†é™çº§ä¿¡æ¯
        fallback_info = fallback_manager.collect_fallback_info(
            required_fallbacks,
            timezone=timezone,
        )
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = fallback_manager.build_system_prompt_with_fallbacks(
            base_prompt_key="workers.general.system",
            fallback_names=required_fallbacks,
            fallback_info=fallback_info,
            language=language,
        )
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{query}"),
        ])
        
        chain = prompt | llm
        result = await chain.ainvoke({
            "query": query,
            "history": history_messages,
            "language": language,
        })
        
        return result.content if hasattr(result, 'content') else str(result)
    
    async def execute(self, state: SupervisorState) -> Dict[str, Any]:
        """æ‰§è¡Œé€šç”¨ä»»åŠ¡"""
        self.log_start("ğŸ’¬")
        
        query = self.get_query(state)
        user_context = self.get_user_context(state)
        
        if not query:
            default_greeting = get_prompt("workers.general.default_greeting")
            return self._create_response(default_greeting, state)
        
        try:
            language = user_context.get("language", "zh-CN")
            timezone = user_context.get("timezone", "Asia/Shanghai")
            language_text = "ä¸­æ–‡" if "zh" in language else "English"
            
            messages = state.get("messages", [])
            history_messages = [
                msg for msg in messages[:-1]
                if isinstance(msg, (HumanMessage, AIMessage))
            ][-6:]
            
            llm = self.get_llm(state)
            
            # å°è¯•ä½¿ç”¨ Function Calling
            if self._tools_supported:
                try:
                    # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ä¼ å…¥å®é™…çš„ language å€¼ï¼Œå¦åˆ™ config.yaml ä¸­çš„ {language}
                    # ä¼šä¿ç•™ä¸ºå ä½ç¬¦ï¼Œå½±å“æ¨¡å‹éµå¾ªè¯­è¨€/é£æ ¼çº¦æŸã€‚
                    system_prompt = get_prompt("workers.general.system", language=language_text)
                    prompt = ChatPromptTemplate.from_messages([
                        ("system", system_prompt),
                        MessagesPlaceholder(variable_name="history"),
                        ("human", "{query}"),
                    ])
                    
                    content = await self._execute_with_tools(
                        llm=llm,
                        prompt=prompt,
                        query=query,
                        history_messages=history_messages,
                        language=language_text,
                        system_prompt=system_prompt,
                    )
                except Exception as e:
                    error_msg = str(e).lower()
                    if "does not support tools" in error_msg or ("tool" in error_msg and "support" in error_msg):
                        logger.warning(f"[{self.name}] æ¨¡å‹ä¸æ”¯æŒ toolsï¼Œåˆ‡æ¢åˆ°é™çº§æ–¹æ¡ˆ")
                        self._tools_supported = False
                        content = await self._execute_without_tools(
                            llm=llm,
                            query=query,
                            history_messages=history_messages,
                            language=language_text,
                            timezone=timezone,
                        )
                    else:
                        raise
            else:
                content = await self._execute_without_tools(
                    llm=llm,
                    query=query,
                    history_messages=history_messages,
                    language=language_text,
                    timezone=timezone,
                )
            
            return self.create_worker_response(
                worker_name=self.name,
                content=content,
                state=state,
            )
            
        except Exception as e:
            logger.error(f"[{self.name}] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return self.create_error_response(
                worker_name=self.name,
                error_message=f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é—®é¢˜: {str(e)}",
                state=state,
            )


# Worker ç±»æ˜ å°„
WORKER_CLASSES = {
    "Researcher": ResearcherWorker,
    "DataAnalyst": DataAnalystWorker,
    "Writer": WriterWorker,
    "General": GeneralWorker,
}


def register_default_workers() -> None:
    """æ³¨å†Œæ‰€æœ‰é»˜è®¤çš„ Worker"""
    from src.router.agents.supervisor.registry import register_worker, get_registry
    
    registry = get_registry()
    
    if not registry.is_empty():
        logger.info("Workers å·²æ³¨å†Œï¼Œè·³è¿‡é‡å¤æ³¨å†Œ")
        return
    
    for worker_class in WORKER_CLASSES.values():
        register_worker(worker_class())
    
    logger.info(f"å·²æ³¨å†Œ {len(WORKER_CLASSES)} ä¸ªé»˜è®¤ Worker")
