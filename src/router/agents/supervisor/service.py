"""
Supervisor Architecture - Service Layer

æœåŠ¡å±‚ï¼Œæä¾› Supervisor Architecture çš„ API æ¥å£ã€‚

åŠŸèƒ½ï¼š
1. æµå¼å’Œéæµå¼è°ƒç”¨
2. é›†æˆ Performance Layerï¼ˆè¯­ä¹‰ç¼“å­˜ + è§„åˆ™å¼•æ“ï¼‰
3. æ”¯æŒç”¨æˆ·ä¸Šä¸‹æ–‡æ³¨å…¥
4. å¢å¼ºçš„æµå¼äº‹ä»¶è¾“å‡ºï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
"""

import time
import json
from typing import AsyncIterator, Dict, Any, Optional, List
from dataclasses import dataclass, field
from enum import Enum
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.base import BaseCheckpointSaver
from src.router.agents.supervisor.workflow import get_graph_app
from src.router.agents.supervisor.supervisor import SupervisorConfig
from src.router.agents.supervisor.state import (
    SupervisorState, 
    DEFAULT_STATE,
    DEFAULT_USER_CONTEXT,
    UserContext,
)
from src.server.logging_setup import logger


class StreamEventType(str, Enum):
    """
    æµå¼äº‹ä»¶ç±»å‹ï¼ˆæç®€ç‰ˆï¼‰
    
    åªå…³æ³¨é‡è¦å†…å®¹ï¼Œä¸æš´éœ²å†…éƒ¨å¤„ç†ç»†èŠ‚
    """
    START = "start"          # å¼€å§‹å¤„ç†
    PROGRESS = "progress"    # å¤„ç†è¿›åº¦ï¼ˆå¯é€‰ï¼‰
    ANSWER = "answer"        # ç­”æ¡ˆ/ç»“æœ
    DONE = "done"            # å®Œæˆ
    ERROR = "error"          # é”™è¯¯


@dataclass
class StreamEvent:
    """
    æµå¼äº‹ä»¶ï¼ˆæç®€ç‰ˆï¼‰
    
    åªè¿”å›ç”¨æˆ·å…³å¿ƒçš„å†…å®¹ï¼š
    - type: äº‹ä»¶ç±»å‹
    - content: å†…å®¹ï¼ˆå¯é€‰ï¼‰
    - progress: è¿›åº¦ï¼ˆå¯é€‰ï¼‰
    """
    type: StreamEventType
    content: str = ""                                # å†…å®¹
    progress: Optional[Dict[str, int]] = None        # {"current": x, "total": y}
    
    def to_dict(self) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºå­—å…¸
        
        è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
        {"type": "answer", "content": "è¿™æ˜¯ç­”æ¡ˆ..."}
        {"type": "progress", "progress": {"current": 1, "total": 2}}
        {"type": "done"}
        """
        result: Dict[str, Any] = {
            "type": self.type.value if isinstance(self.type, StreamEventType) else self.type,
        }
        
        if self.content:
            result["content"] = self.content
        if self.progress:
            result["progress"] = self.progress
        
        return result
    
    def to_sse(self) -> str:
        """è½¬æ¢ä¸º SSE æ ¼å¼"""
        data = json.dumps(self.to_dict(), ensure_ascii=False)
        return f"data: {data}\n\n"


class SupervisorService:
    """
    Supervisor Architecture æœåŠ¡ç±»
    
    æä¾›ç»Ÿä¸€çš„æ¥å£æ¥è°ƒç”¨ Supervisor Architectureã€‚
    
    ç‰¹æ€§ï¼š
    - é›†æˆ Performance Layerï¼ˆå¯é€‰ï¼‰
    - æ”¯æŒç”¨æˆ·ä¸Šä¸‹æ–‡æ³¨å…¥
    - å¢å¼ºçš„æµå¼è¾“å‡ºï¼ˆSSE æ ¼å¼ï¼‰
    - æ€è€ƒè¿‡ç¨‹å®æ—¶æ¨é€
    """
    
    def __init__(
        self,
        checkpointer: Optional[BaseCheckpointSaver] = None,
        supervisor_config: Optional[SupervisorConfig] = None,
        enable_performance_layer: bool = True,
    ):
        """
        åˆå§‹åŒ–æœåŠ¡
        
        Args:
            checkpointer: å¯é€‰çš„æ£€æŸ¥ç‚¹å­˜å‚¨å™¨ï¼Œç”¨äºä¿å­˜å¯¹è¯å†å²
            supervisor_config: å¯é€‰çš„ Supervisor é…ç½®
            enable_performance_layer: æ˜¯å¦å¯ç”¨é€Ÿé€šä¼˜åŒ–å±‚
        """
        self.checkpointer = checkpointer
        self.supervisor_config = supervisor_config
        self.enable_performance_layer = enable_performance_layer
        self._graph_app = None
        self._performance_layer = None
    
    @property
    def graph_app(self):
        """è·å–å›¾åº”ç”¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._graph_app is None:
            self._graph_app = get_graph_app(
                checkpointer=self.checkpointer,
                supervisor_config=self.supervisor_config,
            )
        return self._graph_app
    
    @property
    def performance_layer(self):
        """è·å– Performance Layer å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._performance_layer is None and self.enable_performance_layer:
            try:
                from src.router.agents.performance_layer import get_performance_layer
                self._performance_layer = get_performance_layer()
            except ImportError:
                logger.warning("Performance Layer å¯¼å…¥å¤±è´¥ï¼Œå°†ç¦ç”¨é€Ÿé€šä¼˜åŒ–")
                self._performance_layer = None
        return self._performance_layer
    
    def _build_initial_state(
        self,
        user_message: str,
        user_context: Optional[UserContext] = None,
        initial_state: Optional[Dict[str, Any]] = None,
    ) -> SupervisorState:
        """
        æ„å»ºåˆå§‹çŠ¶æ€
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯
            initial_state: å¯é€‰çš„åˆå§‹çŠ¶æ€
            
        Returns:
            åˆå§‹çŠ¶æ€å­—å…¸
        """
        # åˆå¹¶ç”¨æˆ·ä¸Šä¸‹æ–‡
        context = {**DEFAULT_USER_CONTEXT}
        if user_context:
            context.update(user_context)
        
        return {
            **DEFAULT_STATE,
            "messages": [HumanMessage(content=user_message)],
            "original_query": user_message,
            "user_context": context,
            "metadata": initial_state.get("metadata", {}) if initial_state else {},
        }
    
    def _build_config(self, thread_id: str) -> Dict[str, Any]:
        """æ„å»º LangGraph é…ç½®"""
        return {"configurable": {"thread_id": thread_id}}
    
    def _parse_node_output(
        self,
        node_name: str,
        node_output: Dict[str, Any],
        prev_state: Optional[Dict[str, Any]] = None,
    ) -> List[StreamEvent]:
        """
        è§£æèŠ‚ç‚¹è¾“å‡ºï¼ˆæç®€ç‰ˆï¼‰
        
        åªè¿”å›é‡è¦å†…å®¹ï¼š
        - å®é™…çš„ç­”æ¡ˆ/ç»“æœ (ANSWER)
        - è¿›åº¦æ›´æ–° (PROGRESS) - å¯é€‰
        
        ä¸è¿”å›ï¼š
        - è°åœ¨å¤„ç†
        - å†…éƒ¨å†³ç­–è¿‡ç¨‹
        - ä»»åŠ¡è§„åˆ’ç»†èŠ‚
        """
        events = []
        
        # è®¡ç®—è¿›åº¦ï¼ˆå¯é€‰ï¼‰
        task_plan = node_output.get("task_plan", prev_state.get("task_plan", []) if prev_state else [])
        progress = None
        if task_plan:
            from src.router.agents.supervisor.state import TaskStatus
            completed = sum(1 for step in task_plan if step.get("status") in [TaskStatus.COMPLETED, TaskStatus.SKIPPED])
            total = len(task_plan)
            if total > 1:  # åªæœ‰å¤šæ­¥éª¤ä»»åŠ¡æ‰æ˜¾ç¤ºè¿›åº¦
                progress = {"current": completed, "total": total}
        
        # åªå…³æ³¨ Worker è¾“å‡ºçš„å®é™…å†…å®¹
        if "messages" in node_output and node_name != "supervisor":
            messages = node_output.get("messages", [])
            if messages:
                last_message = messages[-1]
                content = (
                    last_message.content 
                    if hasattr(last_message, 'content') 
                    else str(last_message)
                )
                
                # å‘é€ç­”æ¡ˆ
                events.append(StreamEvent(
                    type=StreamEventType.ANSWER,
                    content=content,
                    progress=progress,
                ))
        
        # å¤šæ­¥éª¤ä»»åŠ¡ï¼šå‘é€è¿›åº¦æ›´æ–°ï¼ˆä¸å«å†…å®¹ï¼‰
        elif node_name == "supervisor" and progress and progress["current"] > 0:
            events.append(StreamEvent(
                type=StreamEventType.PROGRESS,
                progress=progress,
            ))
        
        return events
    
    async def run(
        self,
        user_message: str,
        thread_id: str = "default",
        user_context: Optional[UserContext] = None,
        initial_state: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        éæµå¼è¿è¡Œ Supervisor Architecture
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            thread_id: çº¿ç¨‹ IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯ä¼šè¯
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯
            initial_state: å¯é€‰çš„åˆå§‹çŠ¶æ€
            
        Returns:
            æœ€ç»ˆçŠ¶æ€å­—å…¸
        """
        # 1. æ£€æŸ¥ Performance Layer
        if self.performance_layer:
            cache_result = self.performance_layer.process_query(user_message)
            if cache_result:
                logger.info(f"é€Ÿé€šå±‚å‘½ä¸­ | æ¥æº: {cache_result.get('source')}")
                return {
                    "answer": cache_result.get("answer"),
                    "source": cache_result.get("source"),
                    "cached": True,
                }
        
        # 2. æ„å»ºåˆå§‹çŠ¶æ€
        inputs = self._build_initial_state(user_message, user_context, initial_state)
        config = self._build_config(thread_id)
        
        logger.info(f"ğŸš€ [Supervisor] å¼€å§‹è¿è¡Œ (thread: {thread_id})")
        logger.info(f"   â””â”€ ç”¨æˆ·æ¶ˆæ¯: {user_message[:100]}{'...' if len(user_message) > 100 else ''}")
        
        final_state = None
        try:
            async for event in self.graph_app.astream(inputs, config=config):
                for node_name, node_output in event.items():
                    # æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œ
                    if node_name == "supervisor":
                        logger.info(f"ğŸ“Š [èŠ‚ç‚¹æ‰§è¡Œ] Supervisor èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")
                    elif node_name != "__end__":
                        logger.info(f"ğŸ“Š [èŠ‚ç‚¹æ‰§è¡Œ] {node_name} èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")
                    final_state = node_output
        except Exception as e:
            logger.error(f"è¿è¡Œ Supervisor æ—¶å‡ºé”™: {e}", exc_info=True)
            return {
                "error": str(e),
                "error_type": "execution_error",
            }
        
        # 3. ç¼“å­˜ç»“æœ
        if self.performance_layer and final_state:
            messages = final_state.get("messages", [])
            if messages:
                last_message = messages[-1]
                answer = last_message.content if hasattr(last_message, 'content') else str(last_message)
                self.performance_layer.cache_answer(user_message, answer)
        
        logger.info(f"âœ… [Supervisor] è¿è¡Œå®Œæˆ (thread: {thread_id})")
        return final_state or {}
    
    async def run_stream(
        self,
        user_message: str,
        thread_id: str = "default",
        user_context: Optional[UserContext] = None,
        initial_state: Optional[Dict[str, Any]] = None,
        sse_format: bool = False,
    ) -> AsyncIterator[Dict[str, Any] | str]:
        """
        æµå¼è¿è¡Œ Supervisor Architecture
        
        æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œå®Œåç«‹å³è¿”å›ç»“æœï¼Œæ”¯æŒå®æ—¶åé¦ˆæ€è€ƒè¿‡ç¨‹ã€‚
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            thread_id: çº¿ç¨‹ IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯ä¼šè¯
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯
            initial_state: å¯é€‰çš„åˆå§‹çŠ¶æ€
            sse_format: æ˜¯å¦è¿”å› SSE æ ¼å¼å­—ç¬¦ä¸²ï¼ˆç”¨äº FastAPI StreamingResponseï¼‰
            
        Yields:
            äº‹ä»¶å­—å…¸æˆ– SSE æ ¼å¼å­—ç¬¦ä¸²
        """
        # å‘é€å¼€å§‹äº‹ä»¶
        start_event = StreamEvent(type=StreamEventType.START)
        yield start_event.to_sse() if sse_format else start_event.to_dict()
        
        # 1. æ£€æŸ¥ç¼“å­˜
        if self.performance_layer:
            cache_result = self.performance_layer.process_query(user_message)
            if cache_result:
                # ç›´æ¥å‘é€ç­”æ¡ˆ
                answer_event = StreamEvent(
                    type=StreamEventType.ANSWER,
                    content=cache_result.get("answer"),
                )
                yield answer_event.to_sse() if sse_format else answer_event.to_dict()
                
                done_event = StreamEvent(type=StreamEventType.DONE)
                yield done_event.to_sse() if sse_format else done_event.to_dict()
                return
        
        # 2. æ„å»ºåˆå§‹çŠ¶æ€
        inputs = self._build_initial_state(user_message, user_context, initial_state)
        config = self._build_config(thread_id)
        
        logger.info(f"ğŸš€ [Supervisor] å¼€å§‹æµå¼è¿è¡Œ (thread: {thread_id})")
        logger.info(f"   â””â”€ ç”¨æˆ·æ¶ˆæ¯: {user_message[:100]}{'...' if len(user_message) > 100 else ''}")
        
        prev_state = inputs
        final_answer = ""
        
        try:
            async for event in self.graph_app.astream(
                inputs, 
                config=config, 
                stream_mode="updates"
            ):
                for node_name, node_output in event.items():
                    # æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œ
                    if node_name == "supervisor":
                        logger.info(f"ğŸ“Š [èŠ‚ç‚¹æ‰§è¡Œ] Supervisor èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")
                    elif node_name != "__end__":
                        logger.info(f"ğŸ“Š [èŠ‚ç‚¹æ‰§è¡Œ] {node_name} èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")
                    
                    # è§£æèŠ‚ç‚¹è¾“å‡ºä¸ºäº‹ä»¶
                    stream_events = self._parse_node_output(node_name, node_output, prev_state)
                    
                    for stream_event in stream_events:
                        yield stream_event.to_sse() if sse_format else stream_event.to_dict()
                    
                    # æ›´æ–°å‰ä¸€çŠ¶æ€
                    prev_state = {**prev_state, **node_output}
                    
                    # è®°å½•æœ€ç»ˆç­”æ¡ˆ
                    if "messages" in node_output and node_output["messages"]:
                        last_msg = node_output["messages"][-1]
                        final_answer = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)
                        
        except Exception as e:
            logger.error(f"æµå¼è¿è¡Œ Supervisor æ—¶å‡ºé”™: {e}", exc_info=True)
            error_event = StreamEvent(
                type=StreamEventType.ERROR,
                content=str(e),
            )
            yield error_event.to_sse() if sse_format else error_event.to_dict()
            return
        
        # 3. ç¼“å­˜ç»“æœ
        if self.performance_layer and final_answer:
            self.performance_layer.cache_answer(user_message, final_answer)
        
        # å®Œæˆ
        done_event = StreamEvent(type=StreamEventType.DONE)
        yield done_event.to_sse() if sse_format else done_event.to_dict()
        
        logger.info(f"Supervisor æµå¼è¿è¡Œå®Œæˆ (thread: {thread_id})")
    
    async def get_state(self, thread_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šçº¿ç¨‹çš„å½“å‰çŠ¶æ€
        
        Args:
            thread_id: çº¿ç¨‹ ID
            
        Returns:
            çŠ¶æ€å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        if not self.checkpointer:
            logger.warning("æœªé…ç½® checkpointerï¼Œæ— æ³•è·å–çŠ¶æ€")
            return None
        
        try:
            config = self._build_config(thread_id)
            state = await self.graph_app.aget_state(config)
            return state.values if state else None
        except Exception as e:
            logger.error(f"è·å–çŠ¶æ€æ—¶å‡ºé”™: {e}", exc_info=True)
            return None
    
    async def get_history(self, thread_id: str) -> Optional[list]:
        """
        è·å–æŒ‡å®šçº¿ç¨‹çš„å¯¹è¯å†å²
        
        Args:
            thread_id: çº¿ç¨‹ ID
            
        Returns:
            æ¶ˆæ¯åˆ—è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        state = await self.get_state(thread_id)
        if state:
            return state.get("messages", [])
        return None
    
    def reset_graph(self) -> None:
        """é‡ç½®å›¾åº”ç”¨å®ä¾‹"""
        self._graph_app = None
        logger.info("å·²é‡ç½®æœåŠ¡çš„å›¾åº”ç”¨å®ä¾‹")


# å…¨å±€æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_service_instance: Optional[SupervisorService] = None


def get_service(
    checkpointer: Optional[BaseCheckpointSaver] = None,
    supervisor_config: Optional[SupervisorConfig] = None,
    enable_performance_layer: bool = True,
) -> SupervisorService:
    """
    è·å–å…¨å±€ SupervisorService å®ä¾‹
    
    Args:
        checkpointer: å¯é€‰çš„æ£€æŸ¥ç‚¹å­˜å‚¨å™¨
        supervisor_config: å¯é€‰çš„ Supervisor é…ç½®
        enable_performance_layer: æ˜¯å¦å¯ç”¨é€Ÿé€šä¼˜åŒ–å±‚
        
    Returns:
        SupervisorService å®ä¾‹
    """
    global _service_instance
    if _service_instance is None:
        _service_instance = SupervisorService(
            checkpointer=checkpointer,
            supervisor_config=supervisor_config,
            enable_performance_layer=enable_performance_layer,
        )
    return _service_instance


def reset_service() -> None:
    """é‡ç½®å…¨å±€æœåŠ¡å®ä¾‹"""
    global _service_instance
    _service_instance = None
    logger.info("å·²é‡ç½®å…¨å±€æœåŠ¡å®ä¾‹")
